{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_three_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiB5fKkU8DQpmAXvuGtIA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indhra007/2019/blob/master/final_three_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8olPF7EURtUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nmser(x,y):  #to find normalized mean square error-NMSE\n",
        "    import numpy\n",
        "    z=0 \n",
        "    x1=len(x)\n",
        "    y1=len(y)\n",
        "    #xmean = numpy.mean(x)\n",
        "    if x1==y1:\n",
        "        for k in range(x1):\n",
        "            if x[k] == 0:\n",
        "              x[k] = 0.1\n",
        "            z=z+(((float(x[k])-y[k])**2)/float(x[k]**2))    \n",
        "            \n",
        "    elif x1>>y1 or x1<<y1:\n",
        "        print('invalid list length: please check the list')\n",
        "    \n",
        "    z=z/(x1)\n",
        "    print   (\"NORMALIZED MEAN SQUARE ERROR \")\n",
        "    return (float(z))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRuXj1EXA7Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys \n",
        "if \"../\" not in sys.path:\n",
        "  sys.path.append(\"../\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd451ab8R3mH",
        "colab_type": "code",
        "outputId": "d479e15c-17ac-4bb4-fc69-3c240edfa4bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#*****************************************************\n",
        "#first model - linear regression -with history (three )------------- NORMALIZED MODEL  train data scaled\n",
        "#*****************************************************\n",
        "from numpy.linalg import inv,qr\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "# load dataset\n",
        "data = [i.strip().split() for i in open('final txt data.txt').readlines()]\n",
        "#split dataset\n",
        "train,test=train_test_split(data, shuffle=False, test_size=0.5)\n",
        "\n",
        "# train data\n",
        "res1, res2 = map(list, zip(*train))\n",
        "res1=np.reshape(res1,(-1,1))\n",
        "res2=np.reshape(res2,(-1,1))\n",
        "# data normalize\n",
        "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_x.fit(res1)\n",
        "x11=scaler_x.transform(res1)\n",
        "scaler_y.fit(res2)\n",
        "y11=scaler_y.transform(res2)\n",
        "\n",
        "xr=np.array([float (x) for x in (x11)]) \n",
        "yr=np.array([ float (y) for y in (y11)])\n",
        "# test data\n",
        "testx, testy = map(list, zip(*test))\n",
        "testx=np.reshape(testx,(-1,1))\n",
        "\n",
        "testx=scaler_x.transform(testx)\n",
        "xx=np.array([float (x) for x in (testx)])\n",
        "yoriginal=np.array([ float (y) for y in (testy)])\n",
        "\n",
        "x1=([xr[2:-1],xr[1:-2],xr[0:-3]])\n",
        "x1=np.transpose(x1)\n",
        "print(np.shape(x1),\"x-matrix\")\n",
        "y1=np.array([yr[3:]])\n",
        "y1=np.transpose(y1)\n",
        "print(np.shape(y1),\"y-vector\")\n",
        "# coefficient calc\n",
        "Q, R = qr(x1)  #using qr decomposition method\n",
        "b = (inv(R).dot(Q.T).dot(y1))\n",
        "print(\"coefficient (b)= \",b,\"shape = \",np.shape(b))\n",
        "\n",
        "# ******* train y_pred calc\n",
        "ytrain_p = x1.dot(b) #to predict for train values and check the error between prediction and true value\n",
        "ytrain_pred=scaler_y.inverse_transform(ytrain_p)\n",
        "res222=(res2[3:])\n",
        "res222=([float (x) for x in (res222)])\n",
        "a=(nmser(res222,ytrain_pred))\n",
        "print(\"train data \", a,'\\n')\n",
        "\n",
        "# test y calc\n",
        "x2=([xx[2:-1],xx[1:-2],xx[0:-3]])\n",
        "x2=np.transpose(x2)\n",
        "ytest_p =x2.dot(b)\n",
        "ytest_pred=scaler_y.inverse_transform(ytest_p)\n",
        "testy=(testy[3:])\n",
        "testy=([float (X) for X in testy])\n",
        "print(\"test data \", nmser(testy,ytest_pred) )\n",
        "\n",
        "# saving data to excel file\n",
        "# ytrain_pred=([float (x) for x in ytrain_pred])\n",
        "# dff1=DataFrame()\n",
        "# dff1['train'] = res222\n",
        "# dff1['train_predicted'] = ytrain_pred\n",
        "# dff1.to_excel(r'/content/sample_data/lg_train_pred.xlsx')\n",
        "\n",
        "\n",
        "# ytest_pred=([float (x) for x in ytest_pred])\n",
        "# dff2=DataFrame()\n",
        "# dff2['test']=testy\n",
        "# dff2['test_pred']=ytest_pred\n",
        "#dff2.to_excel(r'/content/sample_data/lg_test_pred.xlsx')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(480, 3) x-matrix\n",
            "(480, 1) y-vector\n",
            "coefficient (b)=  [[-9.05454789e+08]\n",
            " [ 1.81090958e+09]\n",
            " [-9.05454791e+08]] shape =  (3, 1)\n",
            "NORMALIZED MEAN SQUARE ERROR \n",
            "train data  4.086619988398291e-06 \n",
            "\n",
            "NORMALIZED MEAN SQUARE ERROR \n",
            "test data  3.949895386169814e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkr_gmGSGhP",
        "colab_type": "code",
        "outputId": "ab76f91e-12fd-4755-eb80-276db00f9921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#***********************************\n",
        "# second model\n",
        "# model - ARTIFICIAL NEURAL NETWORK\n",
        "#***********************************\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import math\n",
        "from nltk import flatten\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load dataset\n",
        "datafile = [i.strip().split() for i in open('final txt data.txt').readlines()]  # file name/path\n",
        "\n",
        "\n",
        "# split data\n",
        "train,test=train_test_split(datafile,shuffle=False,test_size=0.5)\n",
        "# train data\n",
        "\n",
        "trainx,trainy=map(list,zip(*(train)))\n",
        "\n",
        "trainx=np.array([[float(x)] for x in trainx])\n",
        "trainy=np.array([[float(x)] for x in trainy])\n",
        "\n",
        "trainx_3 = trainy[:(len(trainy)-3)]\n",
        "trainx_2 = trainy[1:(len(trainy)-2)]\n",
        "trainx_1 = trainy[2:(len(trainy)-1)]\n",
        "\n",
        "trainy = trainy[3:]\n",
        "\n",
        "#print(trainy,'\\n','trainy')\n",
        "num=(len(trainx)-3)\n",
        "trainx = np.zeros((num,3))\n",
        "\n",
        "trainx[:,0] = trainx_3.reshape((num))\n",
        "trainx[:,1] = trainx_2.reshape((num))\n",
        "trainx[:,2] = trainx_1.reshape((num))\n",
        "train\n",
        "\n",
        "trainyy=trainy # for calculating rmse error\n",
        "x=trainx\n",
        "y=trainy[:,:]\n",
        "# reshape\n",
        "y=np.reshape(y, (-1,1))\n",
        "#trainx=np.reshape(trainx,(-1,1))\n",
        "trainy=np.reshape(trainy,(-1,1))\n",
        "# data normalize\n",
        "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_x.fit(x)\n",
        "xscale=scaler_x.transform(x)\n",
        "scaler_y.fit(y)\n",
        "yscale=scaler_y.transform(y)\n",
        "#print(yscale,'\\n', 'yscale')\n",
        "# create model with an additional hidden layer\n",
        "model = Sequential()\n",
        "model.add(Dense(669, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(200,activation = 'relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1,activation='linear'))\n",
        "\n",
        "#x_input[:,0] = xscale.reshape((797))\n",
        "#x_input[:,1] = time\n",
        "# compile model\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae','accuracy'])\n",
        "# fit model\n",
        "model.fit(xscale,yscale, epochs=100, batch_size=1,  verbose=1,shuffle=False)\n",
        "\n",
        "# test data\n",
        "testxx,testyy=map(list,zip(*(test)))\n",
        "testx=[[float(x)] for x in testxx]\n",
        "testy=[[float(y)] for y in testyy]\n",
        "\n",
        "# predict data\n",
        "# ----predict train data----\n",
        "\n",
        "testx_3 = list(trainy[(len(trainy)-3):].reshape((3))) + [item for sublist in testy[:(len(testy)-3)] for item in sublist]\n",
        "testx_2 = list(trainy[(len(trainy)-2):].reshape((2))) + [item for sublist in testy[:(len(testy)-2)] for item in sublist]\n",
        "testx_1 = list(trainy[(len(trainy)-1):].reshape((1))) + [item for sublist in testy[:(len(testy)-1)] for item in sublist]\n",
        "\n",
        "\n",
        "testx = np.zeros((484,3))\n",
        "\n",
        "testx[:,0] = testx_3\n",
        "testx[:,1] = testx_2\n",
        "testx[:,2] = testx_1\n",
        "\n",
        "\n",
        "ypred_train=model.predict(xscale)\n",
        "\n",
        "ypred_train1 = scaler_y.inverse_transform(ypred_train)\n",
        "\n",
        "Xnew = np.array(testx)\n",
        "Xnew= scaler_x.transform(Xnew)\n",
        "ynew= model.predict(Xnew)\n",
        "# invert normalize\n",
        "ynew = scaler_y.inverse_transform(ynew) \n",
        "Xnew = scaler_x.inverse_transform(Xnew)\n",
        "trainy= scaler_y.inverse_transform(trainy)\n",
        "\n",
        "\n",
        "xrevcheck=scaler_x.inverse_transform(xscale)\n",
        "yrevcheck=scaler_y.inverse_transform(yscale)\n",
        "\n",
        "# **************** correction needed from here\n",
        "trainScore = math.sqrt(mean_squared_error(trainyy[:], ypred_train[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "\n",
        "testScore = math.sqrt(mean_squared_error(testy[:], ynew[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "# ******************* nmser calc\n",
        "testy=flatten(testy)\n",
        "ynew=([ (x) for x in ynew[:,0]])\n",
        "ytest_new=ynew\n",
        "nmser_calc =nmser(testy, ynew)\n",
        "print(nmser_calc)\n",
        "\n",
        "#plot predictions\n",
        "# #plt.scatter(trainx,trainy)\n",
        "# # ******************* write into excel file\n",
        "# dff3=pd.DataFrame() \n",
        "# dff3['testy']=testy\n",
        "# dff3['ynew_pred']=ynew\n",
        "# dff3.to_excel(r'/content/sample_data/testann.xlsx')\n",
        "del ynew,Xnew,datafile,train,test,ypred_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480 samples\n",
            "Epoch 1/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.4082e-05 - mean_squared_error: 3.4082e-05 - mean_absolute_error: 0.0041 - acc: 0.0042\n",
            "Epoch 2/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 9.5277e-05 - mean_squared_error: 9.5277e-05 - mean_absolute_error: 0.0041 - acc: 0.0042\n",
            "Epoch 3/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.0243e-04 - mean_squared_error: 3.0243e-04 - mean_absolute_error: 0.0055 - acc: 0.0042\n",
            "Epoch 4/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.1540e-04 - mean_squared_error: 2.1540e-04 - mean_absolute_error: 0.0053 - acc: 0.0042\n",
            "Epoch 5/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.2416e-04 - mean_squared_error: 2.2416e-04 - mean_absolute_error: 0.0041 - acc: 0.0042\n",
            "Epoch 6/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.2985e-04 - mean_squared_error: 1.2985e-04 - mean_absolute_error: 0.0031 - acc: 0.0042\n",
            "Epoch 7/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.4671e-04 - mean_squared_error: 2.4671e-04 - mean_absolute_error: 0.0049 - acc: 0.0042\n",
            "Epoch 8/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.8637e-04 - mean_squared_error: 3.8637e-04 - mean_absolute_error: 0.0058 - acc: 0.0042\n",
            "Epoch 9/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.0466e-04 - mean_squared_error: 5.0466e-04 - mean_absolute_error: 0.0073 - acc: 0.0042\n",
            "Epoch 10/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.3165e-04 - mean_squared_error: 4.3165e-04 - mean_absolute_error: 0.0065 - acc: 0.0042\n",
            "Epoch 11/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.5700e-04 - mean_squared_error: 4.5700e-04 - mean_absolute_error: 0.0071 - acc: 0.0042\n",
            "Epoch 12/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.1277e-04 - mean_squared_error: 5.1277e-04 - mean_absolute_error: 0.0069 - acc: 0.0042\n",
            "Epoch 13/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 8.5023e-04 - mean_squared_error: 8.5023e-04 - mean_absolute_error: 0.0098 - acc: 0.0042\n",
            "Epoch 14/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.8836e-04 - mean_squared_error: 6.8836e-04 - mean_absolute_error: 0.0084 - acc: 0.0042\n",
            "Epoch 15/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.1202e-04 - mean_squared_error: 5.1202e-04 - mean_absolute_error: 0.0081 - acc: 0.0042\n",
            "Epoch 16/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.9911e-04 - mean_squared_error: 6.9911e-04 - mean_absolute_error: 0.0087 - acc: 0.0042\n",
            "Epoch 17/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.0035e-04 - mean_squared_error: 3.0035e-04 - mean_absolute_error: 0.0067 - acc: 0.0042\n",
            "Epoch 18/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 9.0718e-04 - mean_squared_error: 9.0718e-04 - mean_absolute_error: 0.0098 - acc: 0.0042\n",
            "Epoch 19/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.3933e-04 - mean_squared_error: 5.3933e-04 - mean_absolute_error: 0.0078 - acc: 0.0042\n",
            "Epoch 20/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.8150e-04 - mean_squared_error: 4.8150e-04 - mean_absolute_error: 0.0072 - acc: 0.0042\n",
            "Epoch 21/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.6757e-04 - mean_squared_error: 1.6757e-04 - mean_absolute_error: 0.0045 - acc: 0.0042\n",
            "Epoch 22/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.7905e-04 - mean_squared_error: 3.7905e-04 - mean_absolute_error: 0.0058 - acc: 0.0042\n",
            "Epoch 23/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.0631e-04 - mean_squared_error: 4.0631e-04 - mean_absolute_error: 0.0055 - acc: 0.0042\n",
            "Epoch 24/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.4225e-04 - mean_squared_error: 2.4225e-04 - mean_absolute_error: 0.0048 - acc: 0.0042\n",
            "Epoch 25/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.1882e-04 - mean_squared_error: 2.1882e-04 - mean_absolute_error: 0.0047 - acc: 0.0042\n",
            "Epoch 26/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.6097e-04 - mean_squared_error: 2.6097e-04 - mean_absolute_error: 0.0050 - acc: 0.0042\n",
            "Epoch 27/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.0112e-04 - mean_squared_error: 2.0112e-04 - mean_absolute_error: 0.0044 - acc: 0.0042\n",
            "Epoch 28/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.5609e-04 - mean_squared_error: 3.5609e-04 - mean_absolute_error: 0.0058 - acc: 0.0042\n",
            "Epoch 29/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.5085e-04 - mean_squared_error: 3.5085e-04 - mean_absolute_error: 0.0058 - acc: 0.0042\n",
            "Epoch 30/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.9008e-04 - mean_squared_error: 2.9008e-04 - mean_absolute_error: 0.0056 - acc: 0.0042\n",
            "Epoch 31/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.8246e-04 - mean_squared_error: 2.8246e-04 - mean_absolute_error: 0.0056 - acc: 0.0042\n",
            "Epoch 32/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.5256e-04 - mean_squared_error: 2.5256e-04 - mean_absolute_error: 0.0048 - acc: 0.0042\n",
            "Epoch 33/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.2903e-04 - mean_squared_error: 1.2903e-04 - mean_absolute_error: 0.0039 - acc: 0.0042\n",
            "Epoch 34/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.6522e-04 - mean_squared_error: 1.6522e-04 - mean_absolute_error: 0.0036 - acc: 0.0042\n",
            "Epoch 35/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.2775e-05 - mean_squared_error: 4.2775e-05 - mean_absolute_error: 0.0022 - acc: 0.0042\n",
            "Epoch 36/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.2407e-05 - mean_squared_error: 6.2407e-05 - mean_absolute_error: 0.0023 - acc: 0.0042\n",
            "Epoch 37/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0505e-04 - mean_squared_error: 1.0505e-04 - mean_absolute_error: 0.0034 - acc: 0.0042\n",
            "Epoch 38/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.7049e-04 - mean_squared_error: 1.7049e-04 - mean_absolute_error: 0.0042 - acc: 0.0042\n",
            "Epoch 39/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0381e-04 - mean_squared_error: 1.0381e-04 - mean_absolute_error: 0.0034 - acc: 0.0042\n",
            "Epoch 40/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.3318e-04 - mean_squared_error: 1.3318e-04 - mean_absolute_error: 0.0033 - acc: 0.0042\n",
            "Epoch 41/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 9.9146e-05 - mean_squared_error: 9.9145e-05 - mean_absolute_error: 0.0031 - acc: 0.0042\n",
            "Epoch 42/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.4404e-04 - mean_squared_error: 1.4404e-04 - mean_absolute_error: 0.0038 - acc: 0.0042\n",
            "Epoch 43/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.2192e-04 - mean_squared_error: 1.2192e-04 - mean_absolute_error: 0.0033 - acc: 0.0042\n",
            "Epoch 44/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.0667e-05 - mean_squared_error: 6.0666e-05 - mean_absolute_error: 0.0024 - acc: 0.0042\n",
            "Epoch 45/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.0585e-05 - mean_squared_error: 6.0585e-05 - mean_absolute_error: 0.0024 - acc: 0.0042\n",
            "Epoch 46/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.0297e-05 - mean_squared_error: 5.0297e-05 - mean_absolute_error: 0.0023 - acc: 0.0042\n",
            "Epoch 47/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 8.7835e-05 - mean_squared_error: 8.7835e-05 - mean_absolute_error: 0.0033 - acc: 0.0042\n",
            "Epoch 48/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0534e-04 - mean_squared_error: 1.0534e-04 - mean_absolute_error: 0.0026 - acc: 0.0042\n",
            "Epoch 49/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.0190e-05 - mean_squared_error: 5.0190e-05 - mean_absolute_error: 0.0021 - acc: 0.0042\n",
            "Epoch 50/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.4204e-05 - mean_squared_error: 5.4204e-05 - mean_absolute_error: 0.0022 - acc: 0.0042\n",
            "Epoch 51/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 8.0539e-05 - mean_squared_error: 8.0539e-05 - mean_absolute_error: 0.0027 - acc: 0.0042\n",
            "Epoch 52/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 8.6880e-05 - mean_squared_error: 8.6880e-05 - mean_absolute_error: 0.0029 - acc: 0.0042\n",
            "Epoch 53/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 9.3971e-05 - mean_squared_error: 9.3971e-05 - mean_absolute_error: 0.0029 - acc: 0.0042\n",
            "Epoch 54/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.0768e-05 - mean_squared_error: 4.0768e-05 - mean_absolute_error: 0.0020 - acc: 0.0042\n",
            "Epoch 55/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.8625e-05 - mean_squared_error: 4.8625e-05 - mean_absolute_error: 0.0020 - acc: 0.0042\n",
            "Epoch 56/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.2798e-05 - mean_squared_error: 5.2798e-05 - mean_absolute_error: 0.0022 - acc: 0.0042\n",
            "Epoch 57/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.0599e-05 - mean_squared_error: 4.0599e-05 - mean_absolute_error: 0.0018 - acc: 0.0042\n",
            "Epoch 58/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.9349e-05 - mean_squared_error: 2.9349e-05 - mean_absolute_error: 0.0015 - acc: 0.0042\n",
            "Epoch 59/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.5059e-05 - mean_squared_error: 1.5059e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 60/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.7254e-05 - mean_squared_error: 1.7254e-05 - mean_absolute_error: 0.0013 - acc: 0.0042\n",
            "Epoch 61/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.1531e-05 - mean_squared_error: 4.1531e-05 - mean_absolute_error: 0.0020 - acc: 0.0042\n",
            "Epoch 62/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.4311e-05 - mean_squared_error: 5.4311e-05 - mean_absolute_error: 0.0023 - acc: 0.0042\n",
            "Epoch 63/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.6475e-05 - mean_squared_error: 5.6475e-05 - mean_absolute_error: 0.0022 - acc: 0.0042\n",
            "Epoch 64/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.8452e-05 - mean_squared_error: 2.8452e-05 - mean_absolute_error: 0.0016 - acc: 0.0042\n",
            "Epoch 65/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.6522e-05 - mean_squared_error: 3.6522e-05 - mean_absolute_error: 0.0017 - acc: 0.0042\n",
            "Epoch 66/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.6870e-05 - mean_squared_error: 2.6870e-05 - mean_absolute_error: 0.0018 - acc: 0.0042\n",
            "Epoch 67/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.2392e-05 - mean_squared_error: 1.2392e-05 - mean_absolute_error: 9.8424e-04 - acc: 0.0042\n",
            "Epoch 68/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.4223e-05 - mean_squared_error: 1.4223e-05 - mean_absolute_error: 0.0013 - acc: 0.0042\n",
            "Epoch 69/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0640e-05 - mean_squared_error: 1.0640e-05 - mean_absolute_error: 0.0011 - acc: 0.0042\n",
            "Epoch 70/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.3659e-05 - mean_squared_error: 2.3659e-05 - mean_absolute_error: 0.0015 - acc: 0.0042\n",
            "Epoch 71/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.8555e-05 - mean_squared_error: 2.8555e-05 - mean_absolute_error: 0.0016 - acc: 0.0042\n",
            "Epoch 72/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.7429e-05 - mean_squared_error: 3.7429e-05 - mean_absolute_error: 0.0018 - acc: 0.0042\n",
            "Epoch 73/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.3802e-05 - mean_squared_error: 3.3801e-05 - mean_absolute_error: 0.0018 - acc: 0.0042\n",
            "Epoch 74/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.5734e-05 - mean_squared_error: 2.5734e-05 - mean_absolute_error: 0.0014 - acc: 0.0042\n",
            "Epoch 75/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.9641e-05 - mean_squared_error: 1.9641e-05 - mean_absolute_error: 0.0013 - acc: 0.0042\n",
            "Epoch 76/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.3803e-05 - mean_squared_error: 1.3803e-05 - mean_absolute_error: 0.0011 - acc: 0.0042\n",
            "Epoch 77/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.1731e-05 - mean_squared_error: 1.1731e-05 - mean_absolute_error: 0.0011 - acc: 0.0042\n",
            "Epoch 78/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.4033e-05 - mean_squared_error: 1.4033e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 79/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.1123e-05 - mean_squared_error: 2.1123e-05 - mean_absolute_error: 0.0014 - acc: 0.0042\n",
            "Epoch 80/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.7682e-05 - mean_squared_error: 2.7682e-05 - mean_absolute_error: 0.0016 - acc: 0.0042\n",
            "Epoch 81/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.8514e-05 - mean_squared_error: 2.8514e-05 - mean_absolute_error: 0.0017 - acc: 0.0042\n",
            "Epoch 82/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.5130e-05 - mean_squared_error: 2.5130e-05 - mean_absolute_error: 0.0015 - acc: 0.0042\n",
            "Epoch 83/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0813e-05 - mean_squared_error: 1.0813e-05 - mean_absolute_error: 0.0011 - acc: 0.0042\n",
            "Epoch 84/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.1815e-05 - mean_squared_error: 1.1815e-05 - mean_absolute_error: 0.0011 - acc: 0.0042\n",
            "Epoch 85/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.9297e-05 - mean_squared_error: 1.9297e-05 - mean_absolute_error: 0.0014 - acc: 0.0042\n",
            "Epoch 86/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.6048e-05 - mean_squared_error: 1.6048e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 87/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.5459e-05 - mean_squared_error: 1.5459e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 88/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.7844e-05 - mean_squared_error: 1.7844e-05 - mean_absolute_error: 0.0013 - acc: 0.0042\n",
            "Epoch 89/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.3646e-05 - mean_squared_error: 1.3646e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 90/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.5057e-05 - mean_squared_error: 1.5057e-05 - mean_absolute_error: 0.0012 - acc: 0.0042\n",
            "Epoch 91/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.1477e-05 - mean_squared_error: 1.1477e-05 - mean_absolute_error: 0.0010 - acc: 0.0042\n",
            "Epoch 92/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 9.1701e-06 - mean_squared_error: 9.1701e-06 - mean_absolute_error: 9.6767e-04 - acc: 0.0042\n",
            "Epoch 93/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.8029e-06 - mean_squared_error: 5.8029e-06 - mean_absolute_error: 7.7363e-04 - acc: 0.0042\n",
            "Epoch 94/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 4.5138e-06 - mean_squared_error: 4.5138e-06 - mean_absolute_error: 7.0851e-04 - acc: 0.0042\n",
            "Epoch 95/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 3.4564e-06 - mean_squared_error: 3.4565e-06 - mean_absolute_error: 6.2433e-04 - acc: 0.0042\n",
            "Epoch 96/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 5.3123e-06 - mean_squared_error: 5.3123e-06 - mean_absolute_error: 8.4823e-04 - acc: 0.0042\n",
            "Epoch 97/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 6.7515e-06 - mean_squared_error: 6.7515e-06 - mean_absolute_error: 7.9088e-04 - acc: 0.0042\n",
            "Epoch 98/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.0180e-05 - mean_squared_error: 1.0180e-05 - mean_absolute_error: 9.5323e-04 - acc: 0.0042\n",
            "Epoch 99/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 2.0992e-06 - mean_squared_error: 2.0992e-06 - mean_absolute_error: 4.9492e-04 - acc: 0.0042\n",
            "Epoch 100/100\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 1.6172e-06 - mean_squared_error: 1.6172e-06 - mean_absolute_error: 4.3810e-04 - acc: 0.0042\n",
            "Train Score: 155.08 RMSE\n",
            "Test Score: 2.31 RMSE\n",
            "NORMALIZED MEAN SQUARE ERROR \n",
            "2.479333770541353e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnGGzQS4SMEE",
        "colab_type": "code",
        "outputId": "20963d7b-5125-488b-d968-3ca4df639bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# *************************************************\n",
        "# THIRD MODEL\n",
        "# LSTM for gold price predictions on monthly basis\n",
        "# *************************************************\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import read_csv\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pandas import DataFrame\n",
        "  \n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset\n",
        "dataframe = read_csv('monthly_csv.csv', usecols=[1], engine='python')\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.5)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1 # for lstm purpose\n",
        "trainX, trainY1 = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "trainY=trainY1\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=1,shuffle=False)\n",
        "# make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "# invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(dataset))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n",
        "#save predicted output\n",
        "# dff6=DataFrame(testY[0], testPredict[:,0]) #\n",
        "# dff6.to_excel(r'/content/sample_data/testlstm.xlsx')\n",
        "\n",
        "# dff5=DataFrame(trainY[0], trainPredict[:,0]) #\n",
        "# dff5.to_excel(r'/content/sample_data/lstm_train.xlsx')\n",
        "\n",
        "nmser_val=nmser(trainY[0],trainPredict)\n",
        "print(nmser_val,\"NORMALIZED MEAN SQUARE ERROR\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "481/481 [==============================] - 2s 4ms/step - loss: 1.6123e-08\n",
            "Epoch 2/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 0.0034\n",
            "Epoch 3/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 9.3621e-04\n",
            "Epoch 4/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.0462e-04\n",
            "Epoch 5/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.7863e-05\n",
            "Epoch 6/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 1.5128e-05\n",
            "Epoch 7/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 7.1814e-06\n",
            "Epoch 8/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.6580e-06\n",
            "Epoch 9/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.5780e-06\n",
            "Epoch 10/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.9730e-06\n",
            "Epoch 11/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.5635e-06\n",
            "Epoch 12/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.2552e-06\n",
            "Epoch 13/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.0106e-06\n",
            "Epoch 14/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.8057e-06\n",
            "Epoch 15/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.6283e-06\n",
            "Epoch 16/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.4799e-06\n",
            "Epoch 17/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.3501e-06\n",
            "Epoch 18/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.2338e-06\n",
            "Epoch 19/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.1307e-06\n",
            "Epoch 20/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 1.0661e-06\n",
            "Epoch 21/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 9.7456e-07\n",
            "Epoch 22/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 8.9333e-07\n",
            "Epoch 23/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 8.1792e-07\n",
            "Epoch 24/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 7.3044e-07\n",
            "Epoch 25/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 6.5788e-07\n",
            "Epoch 26/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 6.3216e-07\n",
            "Epoch 27/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 6.5429e-07\n",
            "Epoch 28/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 6.9359e-07\n",
            "Epoch 29/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 7.2093e-07\n",
            "Epoch 30/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 7.2448e-07\n",
            "Epoch 31/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 7.1046e-07\n",
            "Epoch 32/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 6.8706e-07\n",
            "Epoch 33/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 6.6187e-07\n",
            "Epoch 34/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 6.3878e-07\n",
            "Epoch 35/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 6.1800e-07\n",
            "Epoch 36/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 5.9805e-07\n",
            "Epoch 37/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 5.7829e-07\n",
            "Epoch 38/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 5.5729e-07\n",
            "Epoch 39/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 5.3441e-07\n",
            "Epoch 40/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 5.1089e-07\n",
            "Epoch 41/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.8835e-07\n",
            "Epoch 42/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.6898e-07\n",
            "Epoch 43/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 4.5407e-07\n",
            "Epoch 44/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.4421e-07\n",
            "Epoch 45/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.3743e-07\n",
            "Epoch 46/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.3277e-07\n",
            "Epoch 47/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.2848e-07\n",
            "Epoch 48/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 4.2372e-07\n",
            "Epoch 49/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 4.1800e-07\n",
            "Epoch 50/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 4.1060e-07\n",
            "Epoch 51/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 4.0382e-07\n",
            "Epoch 52/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.9830e-07\n",
            "Epoch 53/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.9290e-07\n",
            "Epoch 54/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.8784e-07\n",
            "Epoch 55/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.8362e-07\n",
            "Epoch 56/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.7962e-07\n",
            "Epoch 57/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.7471e-07\n",
            "Epoch 58/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.6915e-07\n",
            "Epoch 59/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.6393e-07\n",
            "Epoch 60/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.5889e-07\n",
            "Epoch 61/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.5435e-07\n",
            "Epoch 62/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.4957e-07\n",
            "Epoch 63/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.4505e-07\n",
            "Epoch 64/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.3990e-07\n",
            "Epoch 65/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.3507e-07\n",
            "Epoch 66/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.3023e-07\n",
            "Epoch 67/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.2521e-07\n",
            "Epoch 68/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.1996e-07\n",
            "Epoch 69/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.1545e-07\n",
            "Epoch 70/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 3.1154e-07\n",
            "Epoch 71/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.0673e-07\n",
            "Epoch 72/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 3.0253e-07\n",
            "Epoch 73/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.9922e-07\n",
            "Epoch 74/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.9629e-07\n",
            "Epoch 75/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.9381e-07\n",
            "Epoch 76/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.9096e-07\n",
            "Epoch 77/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.8873e-07\n",
            "Epoch 78/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.8677e-07\n",
            "Epoch 79/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.8487e-07\n",
            "Epoch 80/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.8294e-07\n",
            "Epoch 81/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.8106e-07\n",
            "Epoch 82/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7959e-07\n",
            "Epoch 83/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7837e-07\n",
            "Epoch 84/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7748e-07\n",
            "Epoch 85/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7655e-07\n",
            "Epoch 86/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7601e-07\n",
            "Epoch 87/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.7489e-07\n",
            "Epoch 88/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.7420e-07\n",
            "Epoch 89/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.7382e-07\n",
            "Epoch 90/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7333e-07\n",
            "Epoch 91/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.7258e-07\n",
            "Epoch 92/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7243e-07\n",
            "Epoch 93/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7207e-07\n",
            "Epoch 94/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7129e-07\n",
            "Epoch 95/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7113e-07\n",
            "Epoch 96/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7075e-07\n",
            "Epoch 97/100\n",
            "481/481 [==============================] - 1s 3ms/step - loss: 2.7054e-07\n",
            "Epoch 98/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7050e-07\n",
            "Epoch 99/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7060e-07\n",
            "Epoch 100/100\n",
            "481/481 [==============================] - 1s 2ms/step - loss: 2.7038e-07\n",
            "Train Score: 1.59 RMSE\n",
            "Test Score: 29.57 RMSE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhb+IiB9S+IuI+KH/B8teyLWdngEWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "NORMALIZED MEAN SQUARE ERROR \n",
            "0.014227185398340225 NORMALIZED MEAN SQUARE ERROR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLxPhS_sTh-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  **** save all outputs as one\n",
        "\n",
        "\n",
        "from pandas import DataFrame\n",
        "dff_final=DataFrame()\n",
        "\n",
        "ytest_pred=([float (x) for x in ytest_pred])\n",
        "dff_final=DataFrame()\n",
        "dff_final['TRUE']=yoriginal[3:]\n",
        "dff_final['lg_test_pred']=ytest_pred\n",
        "dff_final['ann_test_pred']=ytest_new[3:]\n",
        "dff_final['lstm_test_pred']= testPredict[:,0]\n",
        "#len(yoriginal[3:])==len(ytest_pred)==len(ytest_new[3:])== len(testPredict[:,0])\n",
        "dff_final.to_csv(r'/content/sample_data/finalpred_data.csv',index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ncV0A2Zjw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}